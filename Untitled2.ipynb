{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzHG4rg1gFf7WIU/E8L1AZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LM1997610/Fundamentals_DataScience/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1NycWiyuytbN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 1e-3)\n",
        "        m.bias.data.fill_(0.)\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "#--------------------------------\n",
        "# Device configuration\n",
        "#--------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n",
        "\n",
        "#--------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSkMRyuHyyMr",
        "outputId": "a463ee21-8ff5-4bf7-b0b6-5e8f486594ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------\n",
        "# Hyper-parameters\n",
        "#--------------------------------\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size = [50]\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size = 200\n",
        "learning_rate = 1e-3\n",
        "learning_rate_decay = 0.95\n",
        "reg=0.001\n",
        "num_training= 49000\n",
        "num_validation =1000\n",
        "train = True"
      ],
      "metadata": {
        "id": "iFyhwWS0y2uU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------\n",
        "# Prepare the training and validation splits\n",
        "#-------------------------------------------------\n",
        "mask = list(range(num_training))\n",
        "#train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "#val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n"
      ],
      "metadata": {
        "id": "uia722Rwy3me"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#======================================================================================\n",
        "# Q4: Implementing multi-layer perceptron in PyTorch\n",
        "#======================================================================================\n",
        "# So far we have implemented a two-layer network using numpy by explicitly\n",
        "# writing down the forward computation and deriving and implementing the\n",
        "# equations for backward computation. This process can be tedious to extend to\n",
        "# large network architectures\n",
        "#\n",
        "# Popular deep-learning libraries like PyTorch and Tensorflow allow us to\n",
        "# quickly implement complicated neural network architectures. They provide\n",
        "# pre-defined layers which can be used as building blocks to define our\n",
        "# network. They also enable automatic-differentiation, which allows us to\n",
        "# define only the forward pass and let the libraries perform back-propagation\n",
        "# using automatic differentiation.\n",
        "#\n",
        "# In this question we will implement a multi-layer perceptron using the PyTorch\n",
        "# library.  Please complete the code for the MultiLayerPerceptron, training and\n",
        "# evaluating the model. Once you can train the two layer model, experiment with\n",
        "# adding more layers and report your observations\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Fully connected neural network with one hidden layer\n",
        "#-------------------------------------------------\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, num_classes):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        layers = [nn.Linear(self.input_size, self.hidden_layers),   #Use the layers list to store a variable number of layers\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(self.hidden_layers, self.num_classes),\n",
        "                  nn.Softmax()]\n",
        "\n",
        "        # Enter the layers into nn.Sequential, so the model may \"see\" them\n",
        "        # Note the use of * in front of layers\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #################################################################################\n",
        "        # TODO: Implement the forward pass computations                                 #\n",
        "        # Note that you do not need to use the softmax operation at the end.            #\n",
        "        # Softmax is only required for the loss computation and the criterion used below#\n",
        "        # nn.CrossEntropyLoss() already integrates the softmax and the log loss together#\n",
        "        #################################################################################\n",
        "\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        out1 = self.conv1(self.relu(x1))\n",
        "        out2 = self.conv2(self.relu(x2))\n",
        "\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "EDKBw2ePy7fY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLayerPerceptron(input_size, 10, num_classes).to(device)"
      ],
      "metadata": {
        "id": "R3Y-u55xzLal"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXxWL7eS12wW",
        "outputId": "e86ee611-e9e8-45bf-b2b8-6d09cff7d17a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3072, out_features=100, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  (3): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyP1flLz13ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}